{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Police Shooting Data With Census and Crime Data Covariates\n",
    "\n",
    "This notebook will create two combined datasets and three individual datasets. The goal is to combine as much information at the county level to accompany police shooting databases as to facilitate predictions and data visualizations of police shooting data.\n",
    "\n",
    "The datasets to be created are as follows:\n",
    "\n",
    "1. **[Census data with various county-level features (2014)](#census_df)**\n",
    "2. **[Police Shooting data from 2013-2016](#shooting_df)**\n",
    "3. **[FBI county-level crime data (2014)](#crime_df)**\n",
    "4. **[Combined county-level data with census, police shooting, and crime data](#full_df)**\n",
    "5. **[Police Shooting data with county-level crime and census covariates](#shooting_county_df)**\n",
    "\n",
    "Where possible intermediate datasets are downloaded directly from source. Otherwise they are stored in the ``datasets`` directory. Furthermore, all derived dataframes will be stored in ``derived_datasets``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from urllib2 import Request, urlopen\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "from shapely.geometry import Point, Polygon, MultiPoint, MultiPolygon\n",
    "from shapely.prepared import prep\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='census_df'></a>\n",
    "\n",
    "# 1. Create Census Dataframe\n",
    "\n",
    "Here will use county level census data and shapefiles\n",
    "\n",
    "* **Population demographics**\n",
    "    * County Characteristics Datasets: Annual County Resident Population Estimates by Age, Sex, Race, and Hispanic Origin: April 1, 2010 to July 1, 2015\n",
    "    * https://www2.census.gov/programs-surveys/popest/datasets/2010-2015/counties/asrh/cc-est2015-alldata.csv\n",
    "    \n",
    "* **County level shapefiles**\n",
    "    * Cartographic Boundary Shapefiles - Counties\n",
    "    * http://www2.census.gov/geo/tiger/GENZ2015/shp/cb_2015_us_county_5m.zip\n",
    "    \n",
    "* **Income inequality index (GINI)**    \n",
    "    * 2010-2014 American Community Survey 5-Year Estimates GINI index\n",
    "    * http://factfinder.census.gov/faces/tableservices/jsf/pages/productview.xhtml?pid=ACS_14_5YR_B19083&prodType=table\n",
    "\n",
    "* **Income and Poverty Estimates**\n",
    "    * 2014 Small Area Income and Poverty Estimates, county and state estimates\n",
    "    * https://www.census.gov/did/www/saipe/downloads/estmod14/est14ALL.xls\n",
    "    \n",
    "* **Educational attainment**\n",
    "    * 2010-2014 American Community Survey 5-Year Estimate\n",
    "    * http://factfinder.census.gov/faces/tableservices/jsf/pages/productview.xhtml?pid=ACS_15_5YR_S1501&src=pt\n",
    "* **Unemployment Rate**\n",
    "    * 2014 Local Area Unemployment Statistics (BLS)\n",
    "    *  https://www.bls.gov/lau/laucnty14.xlsx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Census data and make base dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#cdf = pd.read_csv('https://www.census.gov/popest/data/counties/asrh/2015/files/CC-EST2015-ALLDATA.csv')\n",
    "cdf = pd.read_csv('https://www2.census.gov/programs-surveys/popest/datasets/2010-2015/counties/asrh/cc-est2015-alldata.csv')\n",
    "cdf.to_csv('datasets/CC-EST2015-ALLDATA.csv')\n",
    "\n",
    "# select out 2014 data and all ages. We use 2014 since that \n",
    "# is the year of the crime statistics that we will use later\n",
    "cdf = cdf[np.logical_and(cdf['AGEGRP']==0, cdf['YEAR']==7)]\n",
    "\n",
    "# make new column of total FIPS number\n",
    "cdf['FIPS'] = map(lambda x, y: '%02d'%x+'%03d'%y, cdf['STATE'], cdf['COUNTY'])\n",
    "\n",
    "# sort by FIPS and reset index\n",
    "cdf.sort_values('FIPS', inplace=True)\n",
    "cdf.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# hack to get county level shapefiles. I'm sure there is a better python way to do this...\n",
    "cmd = 'mkdir -p datasets/shapefiles/us_county; wget http://www2.census.gov/geo/tiger/GENZ2015/shp/cb_2015_us_county_5m.zip -O datasets/shapefiles/us_county/temp.zip; cd datasets/shapefiles/us_county; unzip temp.zip; rm temp.zip; cd ../../../'\n",
    "os.system(cmd);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Census County data into geopandas dataframe and merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ccnty = gpd.read_file('datasets/shapefiles/us_county/cb_2015_us_county_5m.shp')\n",
    "\n",
    "# define combined fips code\n",
    "ccnty['FIPS'] = ccnty.apply(lambda x: '{0:02d}{1:03d}'.format(int(x['STATEFP']), int(x['COUNTYFP'])), axis=1)\n",
    "\n",
    "# only keep US states + PR\n",
    "statefp = ['01', '02', '04', '05', '06', '08', '09', '10', '11', '12', '13', '15', \n",
    "           '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', \n",
    "           '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', \n",
    "           '40', '41', '42', '44', '45', '46', '47', '48', '49', '50', '51', '53', \n",
    "           '54', '55', '56']\n",
    "\n",
    "ccnty = ccnty[map(lambda x: x in statefp, ccnty['STATEFP'])]\n",
    "\n",
    "# sort by fips code\n",
    "ccnty.sort_values('FIPS', inplace=True)\n",
    "ccnty.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add in population data\n",
    "keys = [u'FIPS', u'STNAME', u'TOT_POP', u'TOT_MALE', u'TOT_FEMALE', u'WA_MALE',\n",
    "       u'WA_FEMALE', u'BAC_MALE', u'BAC_FEMALE', u'AAC_MALE', \n",
    "        u'AAC_FEMALE', u'H_MALE', u'H_FEMALE']\n",
    "ccnty = ccnty.merge(cdf[keys], on='FIPS', how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Race percentages by county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keys = ['WA', 'BAC', 'AAC', 'H']\n",
    "for key in keys:\n",
    "    ccnty[key+'_PER'] = (ccnty[key+'_MALE'] + ccnty[key+'_FEMALE']) / ccnty['TOT_POP']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get GINI data, clean, and merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gini = pd.read_csv('datasets/ACS_14_5YR_B19083_with_ann.csv')\n",
    "gini['FIPS'] = gini.apply(lambda x: '{0:05d}'.format(int(x['FIPS'])), axis=1)\n",
    "gini.rename(columns={'Estimate; Gini Index': 'GINI'}, inplace=True)\n",
    "\n",
    "# merge\n",
    "ccnty = pd.merge(ccnty, gini[['FIPS', 'GINI']], on='FIPS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get income and poverty data, clean, and merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read in excel (why don't they have csv!!!???)\n",
    "incm = pd.read_excel('https://www.census.gov/did/www/saipe/downloads/estmod14/est14ALL.xls', header=3)\n",
    "\n",
    "# output csv for conviencienc\n",
    "incm.to_csv('datasets/est14ALL.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get combined state + county fips\n",
    "incm['FIPS'] = incm.apply(lambda x: '{0:02d}{1:03d}'.format(int(x['State FIPS Code']), \n",
    "                                                            int(x['County FIPS Code'])), axis=1)\n",
    "\n",
    "# replace names\n",
    "keydict = {'Median Household Income': 'median_household_income', 'Poverty Percent, All Ages': 'percent_in_poverty'}\n",
    "incm.rename(columns=keydict, inplace=True)\n",
    "\n",
    "# get rid of blank rows and convert to mean values\n",
    "incm['median_household_income'] = incm['median_household_income'].apply(str)\n",
    "incm['percent_in_poverty'] = incm['percent_in_poverty'].apply(str)\n",
    "incm.replace(to_replace={'median_household_income': {'.': 0}, 'percent_in_poverty': {'.':0}}, inplace=True)\n",
    "incm['median_household_income'] = incm['median_household_income'].apply(int)\n",
    "incm['percent_in_poverty'] = incm['percent_in_poverty'].apply(float)\n",
    "incm.replace(to_replace={'median_household_income': {0: incm['median_household_income'].mean()}, \n",
    "                         'percent_in_poverty': {0:incm['percent_in_poverty'].mean()}}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# merge\n",
    "ccnty = pd.merge(ccnty, incm[['FIPS', 'median_household_income', 'percent_in_poverty']], on='FIPS', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get education attainment data, clean and merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ed = pd.read_csv('datasets/ACS_14_5YR_S1501_with_ann.csv')\n",
    "ed['FIPS'] = ed.apply(lambda x: '{0:05d}'.format(int(x['FIPS'])), axis=1)\n",
    "\n",
    "# just pull out total numbers of high school or higher and bachelor and higher\n",
    "keydict = {'Total; Estimate; Percent high school graduate or higher': 'edu_total_hs', \n",
    "           'Male; Estimate; Percent high school graduate or higher': 'edu_male_hs',\n",
    "           'Female; Estimate; Percent high school graduate or higher': 'edu_female_hs',\n",
    "           'Total; Estimate; Percent bachelor\\'s degree or higher': 'edu_total_bach',\n",
    "           'Male; Estimate; Percent bachelor\\'s degree or higher': 'edu_male_bach',\n",
    "           'Female; Estimate; Percent bachelor\\'s degree or higher': 'edu_female_bach'}\n",
    "ed.rename(columns=keydict, inplace=True)\n",
    "\n",
    "# merge\n",
    "keys = ['FIPS', 'edu_total_hs', 'edu_male_hs', 'edu_female_hs', \n",
    "        'edu_total_bach', 'edu_male_bach', 'edu_female_bach']\n",
    "ccnty = pd.merge(ccnty, ed[keys], on='FIPS', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get unemployment data and merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# explicitly specify column names\n",
    "names = ['laus','stfips','countyfips','cntyname','year', 'blank',\n",
    "         'laborfc','employed','unemployed','unemployment_rate']\n",
    "emp = pd.read_excel('https://www.bls.gov/lau/laucnty14.xlsx', header=6, names=names)\n",
    "\n",
    "# drop NaNs\n",
    "emp.dropna(inplace=True, subset=['stfips','countyfips', 'unemployment_rate'])\n",
    "\n",
    "# get FIPS\n",
    "emp['FIPS'] = map(lambda x, y: '%02d'%int(x)+'%03d'%int(y), emp['stfips'], emp['countyfips'])\n",
    "\n",
    "keys = ['FIPS', 'unemployment_rate']\n",
    "ccnty = pd.merge(ccnty, emp[keys], on='FIPS', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save in csv format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ccnty.to_csv('derived_datasets/county_level_census_data.csv', encoding='utf-8', \n",
    "             index=False, columns=ccnty.columns[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='shooting_df'></a>\n",
    "\n",
    "# 2. Create police shooting dataframe\n",
    "\n",
    "Here we create a dataset from 2013 - 2016 looking at police shooting incidents. We make use of two different datasets for 2013-2014 and 2015-2016\n",
    "\n",
    "* **2015-2016** [Washington Post Police Shooting Database](https://www.washingtonpost.com/graphics/national/police-shootings-2016/)\n",
    "    * 2015: https://s3.amazonaws.com/postgraphics/policeshootings/policeshootings2015.json\n",
    "    * 2016: https://s3.amazonaws.com/postgraphics/policeshootings/policeshootings2016.json\n",
    "* **2013-2014** [Mapping Police Violence Database](http://mappingpoliceviolence.org)\n",
    "    * http://mappingpoliceviolence.org/s/MPVDatasetDownload-4kjm.xlsx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Washington Post data and merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get 2015 and 2016 data from Washington Post\n",
    "df15 = pd.read_json('https://s3.amazonaws.com/postgraphics/policeshootings/policeshootings2015.json')\n",
    "df16 = pd.read_json('https://s3.amazonaws.com/postgraphics/policeshootings/policeshootings2016.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# keep only relevant columns\n",
    "keys = ['name', 'age', 'blurb', 'description', 'date', 'gender', 'race', 'lon', 'lat', 'state', \n",
    "        'armed', 'mental', 'weapon', 'flee', 'is_body_camera']\n",
    "frames = [df15[keys], df16[keys]]\n",
    "dftot = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "# make sure all lowercase\n",
    "keys = ['armed', 'weapon', 'flee', 'blurb', 'description']\n",
    "dftot[keys] = dftot[keys].apply(lambda x: x.str.lower())\n",
    "\n",
    "# maked armed boolean column\n",
    "dftot['armed_bool'] = map(lambda x: x not in ['unarmed'] and x not in ['no weapon'], dftot['armed'].values)\n",
    "\n",
    "# add in year column\n",
    "dftot['year'] = dftot['date'].apply(lambda x: x.year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use county boundaries to determine FIPS for each shooting\n",
    "\n",
    "This is quite expensive and there is probably a better and more efficient way but this seems to work..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define set of Points\n",
    "pts = pd.Series([Point(x, y) for x, y in zip(dftot['lon'].values, dftot['lat'].values)])\n",
    "s_points = MultiPoint(list(pts.values))\n",
    "\n",
    "# define Polygon for each FIPS number\n",
    "fips_polygon = prep(MultiPolygon(list(ccnty['geometry'].values)))\n",
    "\n",
    "# determine points and indices that are within boundaries\n",
    "#fips_points = filter(fips_polygon.contains, pts)\n",
    "fips_points, fips_ind = [], []\n",
    "for ct, fp in enumerate(pts):\n",
    "    if np.any(ccnty['geometry'].map(lambda x: prep(x).contains(fp))):\n",
    "        fips_points.append(fp)\n",
    "        fips_ind.append(True)\n",
    "    else:\n",
    "        fips_ind.append(False)\n",
    "#fips_ind = map(fips_polygon.contains, pts)\n",
    "\n",
    "# get FIPS number corresponding to each shooting\n",
    "fips = map(lambda y: ccnty['FIPS'][ccnty['geometry'].map(lambda x: prep(x).contains(y))].values[0], fips_points)\n",
    "\n",
    "# remap dftot\n",
    "dftot = dftot[fips_ind]\n",
    "dftot['FIPS'] = fips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read, clean, and filter MPV data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mpv = pd.read_excel('http://mappingpoliceviolence.org/s/MPVDatasetDownload-4kjm.xlsx', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### do some cleaning to make responses same as WP data\n",
    "\n",
    "# rename some keys to match WP data\n",
    "keydict = {'Victim\\'s name': 'name', 'Victim\\'s age': 'age', \n",
    "           'Victim\\'s gender': 'gender', 'Victim\\'s race': 'race', \n",
    "           'Date of injury resulting in death (month/day/year)': 'date', \n",
    "           'A brief description of the circumstances surrounding the death': 'description', \n",
    "           'Symptoms of mental illness?': 'mental', 'Unarmed': 'armed', \n",
    "           'Location of death (state)': 'state'}\n",
    "mpv.rename(columns=keydict, inplace=True)\n",
    "\n",
    "# lowercase and filter out\n",
    "mpv['mental'] = mpv['mental'].apply(lambda x: str(x).lower().strip())\n",
    "mpv['mental'] = mpv['mental'].apply(lambda x: x not in ['no', 'unknown', 'nan'])\n",
    "\n",
    "# fill NaN for unknown age\n",
    "mpv.replace(to_replace={'age': {'Unknown':np.nan, 'nan':np.nan}}, inplace=True)\n",
    "\n",
    "# lower description\n",
    "mpv['description'] = mpv['description'].apply(lambda x: unicode(x).lower())\n",
    "\n",
    "# convert dates to datetime objects\n",
    "mpv['date'] = pd.to_datetime(mpv['date'])\n",
    "mpv['year'] = mpv['date'].apply(lambda x: x.year)\n",
    "\n",
    "# convert genders\n",
    "mpv.replace(to_replace={'gender': {'Unknown':np.nan, 'Male':'M', 'Female':'F', \n",
    "                                   'Transgender':np.nan}}, inplace=True)\n",
    "\n",
    "# convert races\n",
    "mpv.replace(to_replace={'race': {'Unknown race':'N', 'White':'W', 'Black':'B', \n",
    "                                 'Hispanic':'H', 'Asian':'A', 'Native American':'O', \n",
    "                                 'Pacific Islander':'O'}}, inplace=True)\n",
    "\n",
    "# get armed status (should probably create a status for unknown)\n",
    "mpv['armed_bool'] = mpv['armed'] != 'Unarmed'\n",
    "\n",
    "# get cause of death, try to separte out shootings since that is all WP data has\n",
    "mpv['shot'] = mpv['Cause of death'].apply(lambda y: 'gunshot' in [x.lower().strip() for x in y.split(',')])\n",
    "\n",
    "# short hand for zip code\n",
    "mpv['zip'] = mpv['Location of death (zip code)']\n",
    "\n",
    "# now filter out all 2015 and 2016 deaths (assume they are in WP data) ...could cross check\n",
    "ind = np.logical_or(mpv['year'].values==2016, mpv['year'].values==2015)\n",
    "mpvf = mpv[~ind]\n",
    "\n",
    "# also filter out all non-gunshot deaths\n",
    "mpvf = mpvf[mpvf['shot'].values==True]\n",
    "\n",
    "# now filter out deaths without zip codes (could do geocoding later but this is ok for counties)\n",
    "mpvf = mpvf[mpvf['zip'].notnull()]\n",
    "\n",
    "# convert zip to str\n",
    "mpvf['zip'] = mpvf['zip'].map(lambda x: '%05d'%int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get zip2fip data and make dictionary\n",
    "z2f = pd.read_excel('https://www.huduser.gov/portal/datasets/usps/ZIP_COUNTY_092016.xlsx')\n",
    "\n",
    "z2f['ZIP'] = map(lambda x: '%05d'%(int(x)), z2f['ZIP'].values)\n",
    "z2f['FIPS'] = map(lambda x: '%05d'%(int(x)), z2f['COUNTY'].values)\n",
    "\n",
    "zips, fips = z2f['ZIP'].values, z2f['FIPS'].values\n",
    "zip2fip = {}\n",
    "for z, f in zip(zips, fips):\n",
    "    zip2fip[z] = f "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get fips code\n",
    "keys = zip2fip.keys()\n",
    "mpvf['FIPS'] = map(lambda x: zip2fip[x] if x in keys else np.nan, mpvf['zip'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine with WP data and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# keep on certain keys\n",
    "keys = ['FIPS', 'name', 'age', 'description', 'date', 'year', 'gender', 'race', 'state', \n",
    "        'armed_bool', 'mental']\n",
    "mpvf = mpvf[keys]\n",
    "\n",
    "dftot = pd.concat([dftot, mpvf], join='outer', axis=0)\n",
    "dftot = dftot[keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dftot.to_csv('derived_datasets/combined_wp_mpv_shooting.csv', encoding='utf-8', \n",
    "             index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='crime_df'></a>\n",
    "# 3. Create FBI Crime dataframe\n",
    "\n",
    "* Uniform Crime Reporting Program Data: Arrests by Age, Sex, and Race, 2014 (ICPSR 36394)\n",
    "    * http://www.icpsr.umich.edu/icpsrweb/NACJD/series/57/studies/36394\n",
    "* Law Enforcement Agency Indentifiers Crosswalk, 2012 (ICPSR 35158)\n",
    "    * http://www.icpsr.umich.edu/icpsrweb/NACJD/studies/35158"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in crime data in tsv format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jaellis/Envs/ps/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "cd = pd.read_table('datasets/36394-0001-Data.tsv', na_values=['99999', '99998'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get FIPS value from FBI Crosswalk file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read in file\n",
    "cross = pd.read_table('datasets/35158-0001-Data.tsv')\n",
    "cross['FIPS'] = cross.apply(lambda x: '{0:05d}'.format(int(x['FIPS'])), axis=1)\n",
    "\n",
    "# make ORI >- FIPS dict\n",
    "ori_fips_dict = {}\n",
    "oris = cross['ORI7'].values\n",
    "fips = cross['FIPS'].values\n",
    "for ori, fip in zip(oris, fips):\n",
    "    ori_fips_dict[ori] = fip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get FIPS code\n",
    "keys = ori_fips_dict.keys()\n",
    "cd['FIPS'] = map(lambda x: ori_fips_dict[x] if x in keys else np.nan, cd['ORI'].values)\n",
    "\n",
    "# filter out data that has fips codes\n",
    "cd = cd[cd['FIPS'].notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create crime categories\n",
    "\n",
    "Create broad crime categories for violent, property, drugs, and weapon crimes. \n",
    "\n",
    "Offense codes from datasets/36394-0001-Codebook.pdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# violent crime (homicide, aggravated assault, forcible rape, robbery)\n",
    "keys = ['01A', '02', '03', '04']\n",
    "ind = cd['OFFENSE'].map(lambda x: x.strip() in keys)\n",
    "cd.ix[ind, 'crime'] = 'violent'\n",
    "\n",
    "# property crime (arson, burglary, larceny-theft, motor vehicle theft)\n",
    "keys = ['05', '06', '07', '09']\n",
    "ind = cd['OFFENSE'].map(lambda x: x.strip() in keys)\n",
    "cd.ix[ind, 'crime'] = 'property'\n",
    "\n",
    "# drug charges\n",
    "keys = ['18', '180', '185', '18A', '18B', '18C', '18D', '18E', \n",
    "        '18F', '18G', '18H']\n",
    "ind = cd['OFFENSE'].map(lambda x: x.strip() in keys)\n",
    "cd.ix[ind, 'crime'] = 'drug'\n",
    "\n",
    "# weapons chages\n",
    "keys = ['15']\n",
    "ind = cd['OFFENSE'].map(lambda x: x.strip() in keys)\n",
    "cd.ix[ind, 'crime'] = 'weapon'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keys = ['FIPS', 'crime', 'AW', 'AB', 'AA', 'JW', 'JB', 'JA', 'AH', 'JH']\n",
    "cd = cd[keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cd.to_csv('derived_datasets/county_crime_data.csv', encoding='utf-8', \n",
    "             index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get arrest counts by race, crime, FIPS code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# groupby fips and crime and sum arrests\n",
    "cd_sum = cd.groupby(['FIPS', 'crime']).agg(np.nansum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get total arrests counts for white, black, asian, hispanic\n",
    "races = ['W', 'B', 'A', 'H']\n",
    "races2 = ['white', 'black', 'asian', 'hispanic']\n",
    "for race, race2 in zip(races, races2):\n",
    "    cd_sum['arrest_count_{0}'.format(race2)] = cd_sum['A{0}'.format(race)] + cd_sum['J{0}'.format(race)]\n",
    "    \n",
    "# filter out columns again\n",
    "keys = ['AW', 'AB', 'AA', 'JW', 'JB', 'JA', 'AH', 'JH']\n",
    "cd_sum.drop(labels=keys, inplace=True, axis=1)\n",
    "\n",
    "# reset index\n",
    "cd_sum.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cd_sum.to_csv('derived_datasets/summed_county_crime_data.csv', encoding='utf-8', \n",
    "             index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='full_df'></a>\n",
    "\n",
    "# 4. Make full county dataframe with census data, shooting data and crime data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add crime data to census data \n",
    "\n",
    "Use census population data to determine the crime rate for the crime categories determined above.\n",
    "\n",
    "**Note:** In some cases, the crime rate is extremely large because of low populations in some counties. Furthermore, I assume the arrest count can have repeat offenders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "crimes = ['property', 'drug', 'violent', 'weapon']\n",
    "races = ['white', 'black', 'hispanic', 'asian']\n",
    "racecodes = ['WA', 'BAC', 'H', 'AAC']\n",
    "\n",
    "\n",
    "## Probably a more clever and more efficient way to do this... ##\n",
    "for crime in crimes:\n",
    "    ccnty['crime_rate_tot_{0}'.format(crime)] = 0.0\n",
    "    for race in races:\n",
    "        ccnty['crime_rate_{0}_{1}'.format(race, crime)] = 0.0\n",
    "        \n",
    "for fips in ccnty['FIPS']:\n",
    "    ind = ccnty['FIPS'] == fips\n",
    "    for crime in crimes:\n",
    "        \n",
    "        # filter out fips code\n",
    "        xf = cd_sum[cd_sum['FIPS'] == fips]\n",
    "        \n",
    "        # filter out crime code\n",
    "        xf = xf[xf['crime']==crime]\n",
    "        \n",
    "        # check to see if any data\n",
    "        if len(xf) > 0:\n",
    "            keys = ['arrest_count_black', 'arrest_count_white', 'arrest_count_hispanic']\n",
    "            arrests = xf[keys].sum(axis=1).values[0]\n",
    "            pop = ccnty[ind]['TOT_POP'].values[0]\n",
    "            if arrests >= 1:\n",
    "                ccnty.ix[ind, 'crime_rate_tot_{0}'.format(crime)] =  arrests / pop * 1000\n",
    "            \n",
    "            for race, rc in zip(races, racecodes):\n",
    "                arrests = xf['arrest_count_{0}'.format(race)].values[0]\n",
    "                pop = (ccnty[ind]['{0}_MALE'.format(rc)]+ccnty[ind]['{0}_FEMALE'.format(rc)]).values[0]\n",
    "                #if arrests/pop > 0.1:\n",
    "                    #print(ccnty[ind]['NAME'].values[0], ccnty[ind]['STNAME'].values[0], fips, arrests, pop, race, \n",
    "                    #       crime, ccnty[ind]['BAC_PER'].values[0])\n",
    "                if arrests >= 1:\n",
    "                    ccnty.ix[ind, 'crime_rate_{0}_{1}'.format(race, crime)] = arrests / pop * 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Shooting data counts to combined data frame\n",
    "\n",
    "Add both counts and counts/yr to get an average shooting rate since 2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get total shootings\n",
    "counts = dftot['FIPS'].value_counts()\n",
    "\n",
    "# avg shooting over years\n",
    "tdiff = (dftot['date'].max() - dftot['date'].min()).days / 365.25\n",
    "counties = pd.DataFrame({'FIPS': counts.index, 'shootings': counts, 'shooting_avg':counts/tdiff})\n",
    "\n",
    "# merge in shooting data\n",
    "ccnty = ccnty.merge(counties, on='FIPS', how='outer')\n",
    "\n",
    "# filter out NaN and replace with 0\n",
    "ccnty.replace(to_replace={'shootings': {np.nan: 0}, 'shooting_avg': {np.nan: 0}}, inplace=True)\n",
    "\n",
    "# get shooting rate per 1,000,000 people\n",
    "ccnty['shooting_rate'] = ccnty['shooting_avg'] / ccnty['TOT_POP'] * 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get shooting by race by armed status\n",
    "races = [('W', 'WA', 'white'), ('B', 'BAC', 'black'), ('H', 'H', 'hispanic'), ('A', 'AAC', 'asian')]\n",
    "armed = ['armed', 'unarmed']\n",
    "for rc, rk, rv in races:\n",
    "    ind = dftot['race']==rc\n",
    "    counts = dftot[ind]['FIPS'].value_counts()\n",
    "    counties = pd.DataFrame({'FIPS': counts.index, 'shootings_{0}_total'.format(rv): counts, \n",
    "                             'shooting_avg_{0}_total'.format(rv): counts/tdiff})\n",
    "    ccnty = ccnty.merge(counties, on='FIPS', how='outer')\n",
    "    ccnty.replace(to_replace={'shootings_{0}_total'.format(rv): {np.nan: 0}, \n",
    "                              'shooting_avg_{0}_total'.format(rv): {np.nan: 0}}, inplace=True)\n",
    "    pop = ccnty['{0}_MALE'.format(rk)] + ccnty['{0}_FEMALE'.format(rk)]\n",
    "    ccnty['shootings_{0}_total_rate'.format(rv)] = ccnty['shooting_avg_{0}_total'.format(rv)] / pop * 1000000\n",
    "    for arm in armed:\n",
    "        ind = np.logical_and(dftot['race']==rc, dftot['armed_bool']==(arm=='armed'))\n",
    "        counts = dftot[ind]['FIPS'].value_counts()\n",
    "        counties = pd.DataFrame({'FIPS': counts.index, 'shootings_{0}_{1}'.format(arm, rv): counts, \n",
    "                                 'shooting_avg_{0}_{1}'.format(arm, rv): counts/tdiff})\n",
    "        ccnty = ccnty.merge(counties, on='FIPS', how='outer')\n",
    "        ccnty.replace(to_replace={'shootings_{0}_{1}'.format(arm, rv): {np.nan: 0}, \n",
    "                                  'shooting_avg_{0}_{1}'.format(arm, rv): {np.nan: 0}}, inplace=True)\n",
    "        pop = ccnty['{0}_MALE'.format(rk)] + ccnty['{0}_FEMALE'.format(rk)]\n",
    "        ccnty['shootings_{0}_{1}_rate'.format(arm, rv)] = ccnty['shooting_avg_{0}_{1}'.format(arm, rv)] / pop * 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ccnty.to_csv('derived_datasets/full_combined_county_data.csv', encoding='utf-8', \n",
    "             index=False, columns=ccnty.columns[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='shooting_county_df'></a>\n",
    "\n",
    "# 5. Add County level data to police shooting dataset \n",
    "\n",
    "This way each shooting will now be accompanied by the corresponding county demographics, census info, and crime statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keys = list(ccnty.columns[24:57])\n",
    "keys.append('FIPS')\n",
    "\n",
    "df = pd.merge(dftot, ccnty[keys], on='FIPS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.to_csv('derived_datasets/shooting_data_with_county_covariates.csv', encoding='utf-8', \n",
    "             index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
